{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Laboratorio 9: Los huesos de Hipócrates 🦴</center></h1>\n",
    "\n",
    "<center><strong>MDS7202: Laboratorio de Programación Científica para Ciencia de Datos</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Matías Rojas y Mauricio Araneda\n",
    "- Auxiliar: Ignacio Meza D.\n",
    "- Ayudante: Rodrigo Guerra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados\n",
    "\n",
    "- Nombre de alumno 1: Andrés Gonzalez Fuentes\n",
    "- Nombre de alumno 2: Camilo Carvajal Reyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Link de repositorio de GitHub:** `https://github.com/camilocarvajalreyes/laboratorios-MDS7202`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indice \n",
    "\n",
    "1. [Temas a tratar](#Temas-a-tratar:)\n",
    "3. [Descripcción del laboratorio](#Descripción-del-laboratorio.)\n",
    "4. [Desarrollo](#Desarrollo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temas a tratar\n",
    "\n",
    "- Creación de clasificadores de imagenes a traves de redes Fully connected y CNN.\n",
    "- Uso de Dataloaders para la carga de datasets.\n",
    "- Comparación de Fully Connected y red convolucional.\n",
    "\n",
    "## Reglas:\n",
    "\n",
    "- Fecha de entrega: 17/11/2022\n",
    "- **Grupos de 2 personas**\n",
    "- **Ausentes** deberán realizar la actividad solos. \n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
    "- Prohibidas las copias. \n",
    "- Pueden usar cualquier material del curso que estimen conveniente.\n",
    "\n",
    "### Objetivos principales del laboratorio\n",
    "\n",
    "- Creación de modelos de clasificación de imágenes utilizando Pytorch.\n",
    "- Creación de dataloader y aplicar transformaciones sobre el dataset.\n",
    "- Comprender la diferencia entre una CNN y una Fully Connected.\n",
    "\n",
    "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones de `Pytorch`, la cual, está enfocada para proyectos de Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos librerias utiles 😸\n",
    "\n",
    "Comenzamos importando librerías utiles para la ejecución del laboratorio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificando los Huesos de Hipócrates🔎\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.pinimg.com/originals/d8/58/66/d85866cd1cc3979f94526551addf74b4.gif\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "Tras el éxito que han tenido proponiendo modelos de machine learning en trabajos anteriores, el famoso medico Hipócrates se ha contactado con ustedes para solicitarles ayuda para automatizar la identificación de radiografías de partes humanas. Para esto, les señala que le gustaría utilizar algoritmos de deep learning producto que Demócrito le señalo que resultan la mejor alternativa para la predicción de imágenes.\n",
    "\n",
    "En su conversación con el medico usted le comenta que ha tenido algunas clases relacionadas a Deep Learning, por esto, están motivados en abordar el problema utilizando redes Fully Connected y redes convolucionales con Pytorch. Sin embargo, al anunciarle los tipos de redes que conocen, el filósofo les comenta que no había escuchado muy buenos resultados por parte de las CNN, por lo que les pide que comprueben a traves de la métrica de accuracy que tipo de redes es mejor para la tarea de identificación de radiografías. ¿Será cierto lo que dice el filósofo?, Veámoslo en un nuevo capítulo de los Laboratorios de Programación Científica para Ciencia de Datos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creación de Lista de Archivos\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.tenor.com/BJ-9w-MUVCMAAAAM/tis100-sad.gif\" width=\"300\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comience revisando de forma manual cada una de las imágenes que posee la carpeta subida a material docente. Verifique la cantidad de tipos de radiografías que se tienen y la cantidad de imágenes que dispone cada carpeta.\n",
    "\n",
    "Revisado el contenido de las imágenes, utilice `os.listdir` para crear un `numpy.array` o un `Dataframe` que contenga las imágenes y un label que señale al tipo de radiografía a la que hace referencia la imagen. Para hacer las etiquetas codifique el tipo de imágenes en números que vayan del 0 al total de tipos de radiografías, no utilice strings para codificar las etiquetas.\n",
    "\n",
    "**Ejemplo de Estructura:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\"></th>\n",
    "    <th class=\"tg-0lax\">image_path</th>\n",
    "    <th class=\"tg-0lax\">label</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">0</td>\n",
    "    <td class=\"tg-0lax\">image1</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">image2</td>\n",
    "    <td class=\"tg-0lax\">0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\">image3</td>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">3</td>\n",
    "    <td class=\"tg-0lax\">image4</td>\n",
    "    <td class=\"tg-0lax\">0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">4</td>\n",
    "    <td class=\"tg-0lax\">image5</td>\n",
    "    <td class=\"tg-0lax\">4</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AbdomenCT/001857.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AbdomenCT/007792.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AbdomenCT/007577.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AbdomenCT/002785.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AbdomenCT/005169.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_path  label\n",
       "0  AbdomenCT/001857.jpeg      0\n",
       "1  AbdomenCT/007792.jpeg      0\n",
       "2  AbdomenCT/007577.jpeg      0\n",
       "3  AbdomenCT/002785.jpeg      0\n",
       "4  AbdomenCT/005169.jpeg      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i, category in enumerate(os.listdir(\"Medical-MNIST\")):\n",
    "    for image in os.listdir(f\"Medical-MNIST/{category}\"):\n",
    "        X.append(f\"{category}/{image}\")\n",
    "        y.append(i)\n",
    "\n",
    "xray_df = pd.DataFrame({'image_path':X, 'label':y})\n",
    "xray_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Creación de Dataset\n",
    "\n",
    "Tomando en cuenta la estructura de datos desarrollada en el punto 1.1, construya la clase `MedicalDataset()` que cumpla los siguientes puntos:\n",
    "\n",
    "- [X] Poseer un `__init__` en el que se almacene `estructura` creada en 1.1, la `raiz` de la carpeta y una función que permita transformar el dataset (de esto no se preocupe mucho, ya que solamente debe almacenar una función en el atributo).\n",
    "- [X] La clase debe ser capaz de entregar la cantidad de elementos a traves de `__len__`.\n",
    "- [X] Debe poseer el método `__getitem__` que retorne una tupla con la imagen y su correspondiente etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código Aquí\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, estructura, raiz, transform):\n",
    "        self.estructura = estructura\n",
    "        self.raiz = raiz\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Un poco de ayuda para cargar la imagen\n",
    "        img_path = f\"{self.raiz}/{self.estructura.image_path[idx]}\"\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.estructura.label[idx]\n",
    "        \n",
    "        # Auida para realizar la transformación\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.estructura.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Prueba del MedicalDataset\n",
    "\n",
    "Con la clase construida en el punto 1.2, verifique su funcionamiento cargando el dataset y realizando las transformaciones que entrega la función `transform_image`. Compruebe a través de un ejemplo las transformaciones aplicadas en la imagen, comentando la función que cumple `MedicalDataset` y si es posible observar todas las transformaciones aplicadas con la función `transform_image`.\n",
    "\n",
    "- [X] Probar la clase MedicalDataset y aplicando una transformación de \"train\"\n",
    "- [X] Plotear un ejemplo del MedicalDataset.\n",
    "\n",
    "**Función para transformar las imagenes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 64\n",
    "\n",
    "def transform_image(stage = None):\n",
    "    \n",
    "    if stage == \"train\":\n",
    "        # Tr_img = T.Compose([T.Resize(size = (256,256)),\n",
    "        Tr_img = T.Compose([T.Resize(size = (dim,dim)),\n",
    "                T.RandomRotation(degrees = (-20,+20)),\n",
    "                T.ToTensor()])\n",
    "        \n",
    "    elif stage == \"test\" or stage == \"val\":\n",
    "        # Tr_img = T.Compose([T.Resize(size = (224,224)), T.ToTensor()]) \n",
    "        Tr_img = T.Compose([T.Resize(size = (dim,dim)), T.ToTensor()]) \n",
    "\n",
    "    return Tr_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Código para obtener un ejemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba del dataset\n",
    "dataset = MedicalDataset(xray_df, \"Medical-MNIST\", transform_image(\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x: %{x}<br>y: %{y}<extra></extra>",
         "name": "0",
         "source": "data:image/png;base64,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",
         "type": "image",
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_test= iter(dataset)\n",
    "example_image = next(data_test)\n",
    "\n",
    "# Utilice plotly para plotear un ejemplo\n",
    "px.imshow(example_image[0][0], binary_string=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comente que realiza la clase construida y las transformaciones aplicadas.\n",
    "```\n",
    "La clase construida se encarga de crear un dataset que luego pueda ser implementado en conjunto con un Dataloader de Pytorch, para lo que tiene que contar con los métodos '__getitem__' y '__len__' para entregar un elemento del dataset iterativamente según un índice y para contar con el largo de todo el dataset. Luego, las transformaciones aplicadas sirven para reajustar una imagen a un tamaño estándar de 256x256 y luego una pequeña rotación random, seguramente para entrenar un modelo que luego sea robusto a las rotaciones. Finalmente se hace la transformación a un tensor de Pytorch, para que pueda ser utilizado en un modelo.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Creación de Clasificadores\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://149695847.v2.pressablecdn.com/wp-content/uploads/2018/01/conv-full-layer.gif\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "A continuación, deben construir tres clasificadores con los que deberán verificar cuál de las arquitecturas posee un mejor desempeño para la tarea de clasificación de imágenes. Para la construcción considere los siguientes puntos:\n",
    "\n",
    "- [X] Señale cual es el objetivo del `forward` en este tipo de redes, sea breve para su explicación.\n",
    "- [X] Construir una red Fully Connected para solucionar el problema de clasificación. Para esta parte se le aconseja que rellene el esqueleto dispuesto más abajo y que lleva el nombre de `FCClassifier`, en el deberá rellenar con la dimensión de las capas ocultas y verificar cual será el tamaño de la entrada.\n",
    "- [X] Construya una red convolucional **simple** (no más de una capa convolucional) para la tarea de clasificación de imágenes, para esto basen su código en la clase del día `09-11-2022`.\n",
    "- [X] Crear una red convolucional más compleja. Para esta parte tienen completa libertad en la construcción de su red, lo único que debe cumplir es que sea convolucional.\n",
    "\n",
    "**Esqueletos Propuestos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCClassifier(nn.Module):\n",
    "    def __init__(self, in_dimension, num_classes):\n",
    "        super(FCClassifier, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lin1 = nn.Linear(in_dimension, 64)\n",
    "        self.lin2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        return self.lin2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier1(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(CNNClassifier1, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            )\n",
    "        self.fc = nn.Linear(in_features=32 * 128 * 128, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return self.fc(x.flatten())\n",
    "\n",
    "    \n",
    "class CNNClassifier2(nn.Module):\n",
    "    def __init__(self, channel_list, num_classes):\n",
    "        super(CNNClassifier2, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for idx in range(1, len(channel_list)):\n",
    "            if idx % 2 ==0:\n",
    "                layer = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=channel_list[idx-1], out_channels=channel_list[idx], kernel_size=3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(channel_list[idx]),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "            else:\n",
    "                layer = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=channel_list[idx-1], out_channels=channel_list[idx], kernel_size=3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(channel_list[idx]),\n",
    "                    nn.ReLU())\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(64 * 64 * channel_list[-1], 1024),\n",
    "                    nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(1024, 1024),\n",
    "                    nn.ReLU())                    \n",
    "        self.fc3 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.fc1(x.flatten())\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Respuesta\n",
    "```\n",
    "El objetivo de el método 'forward' es hacer el cálculo de las salidas de cada capa dentro de un modelo a partir de sus entradas, realizando un paso de los gradientes en aquellos tensores entrenables para posteriormente propagar los errores con backpropagation. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Separando Datos para el Entrenamiento\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://c.tenor.com/Esn7Jif-ZLQAAAAC/separate-square.gif\" width=\"200\">\n",
    "</p>\n",
    "\n",
    "Utilizando un Holdout a su gusto, separe los datos en un conjunto de entrenamiento y de testing. Aplique las transformaciones correspondientes usando `transform_image` para cada conjunto de datos y utilice `torch.utils.data.DataLoader` para crear un objeto iterable del dataset.\n",
    "\n",
    "- [X] Definir el Holdout a utilizar.\n",
    "- [X] Separar los datos en un conjunto de entrenamiento y prueba.\n",
    "- [X] Aplicar las transformaciones correspondientes en cada uno de los dataset.\n",
    "- [X] Utilizar `DataLoader` de pytorch sobre los dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar Datos de Entrenamiento\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.3\n",
    "X_train, X_test = train_test_split(xray_df, test_size=test_size, random_state=42)\n",
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()\n",
    "X_train = MedicalDataset(X_train, \"Medical-MNIST\", transform_image(\"train\"))\n",
    "X_test = MedicalDataset(X_test, \"Medical-MNIST\", transform_image(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de torch.utils.data.DataLoader\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    X_train,\n",
    "    batch_size = batch_size,\n",
    "    pin_memory = True,\n",
    "    num_workers = 12,\n",
    "    shuffle = True\n",
    "    )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    X_test,\n",
    "    batch_size = batch_size,\n",
    "    pin_memory = True,\n",
    "    num_workers = 0,\n",
    "    shuffle = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Creación de Funciones de Entrenamiento y Evaluación\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.researchgate.net/publication/319535615/figure/fig3/AS:536187598065664@1504848493070/A-typical-convolutional-neural-network-CNN-Architecture-for-Medical-Image-Classification.png\" width=\"500\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Ya construido todas las funciones y clases necesarias llego el momento más importante... probar la red. Para esta sección, ustedes deberán ser capaces de definir los hiperparámetros de la red, definir las funciones de perdida a utilizar, señalar el optimizador a usar y finalmente crear sus funciones para el entrenamiento y prueba. Para realizar esta parte más estructurada, seguir los siguientes puntos de forma secuencial:\n",
    "\n",
    "- [ ] Especifique los Hiperparámetros de las 3 redes. Para esta parte sea claro de su elección y señale el porqué de sus elecciones (o sea justifique el setting de sus hiperparámetros).\n",
    "- [x] Defina los modelos a utilizar, el optimizador que utilizará para el modelo y señale la función de perdida que utilizará.\n",
    "- [x] Explique de forma breve la función que cumplen los pasos `Backward` y `Descenso del gradiente` en una red neuronal.\n",
    "- [x] Cree una función llamado `train` que entrene a los clasificadores. Para esto, recuerde que estos modelos suelen utilizar un número de épocas, por lo que deberá generar un proceso iterativo de entrenamiento. Es importante que su función imprima las `loss` obtenidas por el modelo en cada época (si gusta puede almacenar estas losses en una lista para luego graficarlas y comparar).\n",
    "- [x] Diseñe una función para evaluar el desempeño de las redes. Para evaluar las redes utilice solamente la métrica accuracy (para esto se le recomienda comparar la predicción con el ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificar hyperparámetros de las redes\n",
    "# dim = 64  # dimensión del reshape al pre-procesar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6\n",
    "in_dimension = dim * dim * 3\n",
    "# Red 1\n",
    "model_fc = FCClassifier(in_dimension, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels_cnn = 3\n",
    "# Red 2\n",
    "model_CNN1 = CNNClassifier1(in_channels_cnn, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_list_cnn2 = [3,dim,dim,128,128,256]\n",
    "# Red 3\n",
    "model_CNN2 = CNNClassifier2(channels_list_cnn2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = [model_fc, model_CNN1, model_CNN2]\n",
    "# modelos = [model_fc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parámetros de entrenamiento\n",
    "lr = 1e-4\n",
    "n_epochs = 5  # 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Explicación parámetros\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Para el criterio elegimos la pérdida de entropía cruzada. Esta corresponde a la elección natural en un problema de clasificación multimodal y equivale a la minimización del estimador de máxima verosimilitud.\n",
    "\n",
    "Por otro lado, escogemos el optimizador Adam, que es una variación de SGD ampliamente utilizado y que combina elementos de AdaGrad y RMSProp.\n",
    "```\n",
    "\n",
    "Nota: redefiniremos el optimizador dentro de la función entrenar. Esto pues el optimizador guarda parámetros de cada modelo en particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Backward:\n",
    "Durante el entrenamiento, una vez que la información pasa a través de la red neuronal para computar una predicción, se computará un error (forward). Enseguida, se computa\n",
    "el gradiente usando las operaciones que estaban guardadas y mediante el uso exhaustivo de la regla de la cadena, haciendo fluir la información en reversa hasta tener el valor de cada derivada parcial (backward).\n",
    "\n",
    "\n",
    "Descenso del gradiente (estocástico):\n",
    "Esta parte de las iteraciones aplica el paso del algoritmo descenso de gradiente, o como en este caso, una variación de este. Para esto, toma el gradiente calculado en el paso backward y los resta a los valores de los parámetros de nuestra red escalado por el learning rate. El paso posterior .zero_grad() \"limpia\" el optimizador de los errores de pasos anteriores.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(model, train_loader, num_epochs, verbose=True):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    perdidas = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        perdidas_epoch = []\n",
    "        for i, (data, label) in enumerate(train_loader):\n",
    "            #Forward\n",
    "            pred = model(data)\n",
    "            \n",
    "            #Backward\n",
    "            loss = criterion(pred,label)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Descenso del gradiente\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            perdidas_epoch.append(loss.item())\n",
    "\n",
    "        perdidas += perdidas_epoch\n",
    "\n",
    "        if verbose:\n",
    "            print(\"epoca {} de {}, training loss = {}\".format(e+1,num_epochs,perdidas[-1]))\n",
    "            print(\"--------------------------------------------\")\n",
    "\n",
    "    return perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import accuracy_score\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=0)\n",
    "\n",
    "def evaluate(loader, model, verbose=True):\n",
    "    y_pred, y_true = [], []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, labels) in enumerate(loader):\n",
    "            output = model(data)\n",
    "            y_pred.append(torch.argmax(logsoftmax(output),axis=1).detach().numpy())\n",
    "\n",
    "            y_true.append(labels.detach().numpy())\n",
    "    \n",
    "    score =  accuracy_score(np.concatenate(y_pred), np.concatenate(y_true))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Accuracy del modelo {}: {}\".format(model.__class__.__name__,score))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo incluimos el accuracy de el modelo feed-forwards sin entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo FCClassifier: 0.09521117204726635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09521117204726635"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_loader,model_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y122sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m evaluate(test_loader,model_CNN1)\n",
      "\u001b[1;32m/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb Cell 44\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(loader, model, verbose)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y122sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (data, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y122sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y122sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         y_pred\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39;49margmax(logsoftmax(output),axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y122sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         y_true\u001b[39m.\u001b[39mappend(labels\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y122sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m score \u001b[39m=\u001b[39m  accuracy_score(np\u001b[39m.\u001b[39mconcatenate(y_pred), np\u001b[39m.\u001b[39mconcatenate(y_true))\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "evaluate(test_loader,model_CNN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Comparación de Resultados\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media2.giphy.com/media/icJA0VF7ntoEL18Jez/giphy.gif\"  width=\"200\">\n",
    "</p>\n",
    "\n",
    "Construidas las funciones de entrenamiento y evaluación, entrene a las redes que construyo y compare los resultados obtenidos con todas las redes señalando cual posee mejor rendimiento. Comente una diferencia entre las redes Fully Connected y CNN podría generar un mejor desempeño en una u otra en la tarea de clasificación de imágenes.\n",
    "\n",
    "- [ ] Entrenar las redes.\n",
    "- [ ] Evaluar las redes.\n",
    "- [ ] Comentar los resultados obtenidos y visualizar si existe una diferencia significativa en el rendimiento debido a la naturaleza de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "perdidas = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento de modelo FCClassifier\n",
      "epoca 1 de 5, training loss = 0.2586401402950287\n",
      "--------------------------------------------\n",
      "epoca 2 de 5, training loss = 0.023460598662495613\n",
      "--------------------------------------------\n",
      "epoca 3 de 5, training loss = 0.03540148213505745\n",
      "--------------------------------------------\n",
      "epoca 4 de 5, training loss = 0.0019388062646612525\n",
      "--------------------------------------------\n",
      "epoca 5 de 5, training loss = 0.006907980423420668\n",
      "--------------------------------------------\n",
      "CPU times: user 9min 6s, sys: 17.3 s, total: 9min 23s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Entrenamiento de modelo FCClassifier\")\n",
    "perdidas[\"FCClassifier\"] = train(model_fc,train_loader,n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Entrenamiento de modelo CNNClassifier1\")\n",
    "perdidas[\"CNNClassifier1\"] = train(model_CNN1,train_loader,n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Entrenamiento de modelo CNNClassifier2\")\n",
    "perdidas[\"CNNClassifier2\"] = train(model_CNN2,train_loader,n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_tag = [model.__class__.__name__ for model in modelos]\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "_, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "for model in y_tag:\n",
    "    sns.lineplot(data=perdidas[model], ax=ax, markers=['o','o','o'])\n",
    "ax.legend(y_tag)\n",
    "ax.set(xlabel='iteración',ylabel='error',title='Error versus iteraciones para distintos modelos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo FCClassifier: 0.8956860971334879\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb Cell 50\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m modelos:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     evaluate(test_loader,model)\n",
      "\u001b[1;32m/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb Cell 50\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(loader, model, verbose)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y106sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (data, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y106sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y106sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         y_pred\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39;49margmax(logsoftmax(output),axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y106sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         y_true\u001b[39m.\u001b[39mappend(labels\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/laboratorios-MDS7202/Lab_9/Lab_9.ipynb#Y106sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m score \u001b[39m=\u001b[39m  accuracy_score(np\u001b[39m.\u001b[39mconcatenate(y_pred), np\u001b[39m.\u001b[39mconcatenate(y_true))\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "for model in modelos:\n",
    "    evaluate(test_loader,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comente los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.tenor.com/vKSR-ZakVMIAAAAC/pochitadancing-pochita.gif\">\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('prog_cientifica')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f12cc99d524414e3daaca7ff7e8e0997cdecd19277513297c62237218c0d43b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
