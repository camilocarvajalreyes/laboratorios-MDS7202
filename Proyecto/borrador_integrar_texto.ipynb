{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Borrador integración o no de variable con descripción de texto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea:\n",
    "- tener un pipeline básico para cada una de las tareas\n",
    "- dijar pre-procesamiento\n",
    "- compatibilidad con output de modelo de lenguaje\n",
    "- elegir mejor manera de incluir modelo de lenguaje"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Columnas con categorías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class CategoriesTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return doc.split(';')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta versión de vectorizador es para columnas con pocas categorías posibles (<1k):\n",
    "- platforms (3 valores posibles)\n",
    "- categories (29 valores posibles)\n",
    "- genres (26 valores posibles)\n",
    "- tags (306 valores posibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boc_some_values = CountVectorizer(\n",
    "    tokenizer = CategoriesTokenizer(),\n",
    "    max_df = 1.0,\n",
    "    min_df = 0.05  # hiperparametro a optimizar\n",
    "    # valores para GridSearch : [5%, 10%, 15%] ???\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta otra versión es para developers y publishers (5617 y 3961 valores posibles respectivamente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boc_many_values = CountVectorizer(\n",
    "    tokenizer = CategoriesTokenizer(),\n",
    "    max_df = 1.0,\n",
    "    min_df = 1  # hiperparametro a optimizar\n",
    "    # valores para GridSearch : [5, 10, 15] ???\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable de fecha de publicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def custom_features(dataframe_in):\n",
    "    df = dataframe_in.copy(deep=True)\n",
    "\n",
    "    df['month'] = pd.to_datetime(df['release_date']).dt.month\n",
    "    df['release_date'] = pd.to_datetime(df['release_date']).apply(lambda x: x.to_julian_date())\n",
    "\n",
    "    df['revenue'] = pd.Series([0 for _ in range(len(dataframe_in))])\n",
    "\n",
    "    df.loc[df.publisher.str.match('.*microsoft.*', flags=re.IGNORECASE).values, 'revenue'] = 10.260\n",
    "    df.loc[df.publisher.str.match('.*netease.*', flags=re.IGNORECASE).values, 'revenue'] = 6.668\n",
    "    df.loc[df.publisher.str.match('.*activision.*', flags=re.IGNORECASE).values, 'revenue'] = 6.388\n",
    "    df.loc[df.publisher.str.match('.*electronic.*', flags=re.IGNORECASE).values, 'revenue'] = 5.537\n",
    "    df.loc[df.publisher.str.match('.*bandai.*', flags=re.IGNORECASE).values, 'revenue'] = 3.018\n",
    "    df.loc[df.publisher.str.match('.*square.*', flags=re.IGNORECASE).values, 'revenue'] = 2.386\n",
    "    df.loc[df.publisher.str.match('.*nexon.*', flags=re.IGNORECASE).values, 'revenue'] = 2.286\n",
    "    df.loc[df.publisher.str.match('.*ubisoft.*', flags=re.IGNORECASE).values, 'revenue'] = 1.446\n",
    "    df.loc[df.publisher.str.match('.*konami.*', flags=re.IGNORECASE).values, 'revenue'] = 1.303\n",
    "    df.loc[df.publisher.str.match('.*SEGA.*').values, 'revenue'] = 1.153\n",
    "    df.loc[df.publisher.str.match('.*capcom.*', flags=re.IGNORECASE).values, 'revenue'] = 0.7673\n",
    "    df.loc[df.publisher.str.match('.*warner.*', flags=re.IGNORECASE).values, 'revenue'] = 0.7324\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Juntando todo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer, OneHotEncoder\n",
    "\n",
    "\n",
    "preprocesisng = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('BoC-plat',boc_some_values,'platforms'),\n",
    "        ('BoC-cat',boc_some_values,'categories'),\n",
    "        ('BoC-genres',boc_some_values,'genres'),\n",
    "        ('BoC-tags',boc_some_values,'tags'),\n",
    "\n",
    "        ('BoC-dev',boc_many_values,'developer'),\n",
    "        ('BoC-pub',boc_many_values,'publisher'),\n",
    "\n",
    "        ('OneHotEncoder',OneHotEncoder(handle_unknown='ignore'),['month']),\n",
    "        # ('StandardScaler',StandardScaler(), ['...']),\n",
    "        ('MinMaxScaler',MinMaxScaler(),['required_age','price','release_date']),\n",
    "        ('BoxCox',PowerTransformer(method='yeo-johnson'),['achievements','average_playtime','revenue']),\n",
    "        # ('unchanged',None,['english'])  # chequear como no hacer nada\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('Pre-procesamiento',preprocesisng),\n",
    "    (\"selector\", SelectPercentile(f_classif, percentile=95)),\n",
    "    ('Clasificador',MLPClassifier(early_stopping =True,max_iter = 100, random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = pd.read_pickle('train.pickle')\n",
    "df_train = custom_features(df_train)\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(df_train, df_train['rating'], test_size=0.3, random_state=0, stratify=df_train['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time_ns()\n",
    "    s = now - since\n",
    "    return s*10**(-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados clasificación MLP\n",
      "Time elapsed for training: 6.620332166000001 seconds\n",
      "\n",
      "Time elapsed for inference (eval set): 0.043536191 seconds\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.31      0.28      0.29       497\n",
      "Mostly Positive       0.28      0.29      0.28       512\n",
      "       Negative       0.46      0.33      0.38       387\n",
      "       Positive       0.33      0.42      0.37       610\n",
      "  Very Positive       0.41      0.36      0.38       359\n",
      "\n",
      "       accuracy                           0.34      2365\n",
      "      macro avg       0.36      0.34      0.34      2365\n",
      "   weighted avg       0.35      0.34      0.34      2365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Resultados clasificación MLP\")\n",
    "start = time.time_ns()\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"Time elapsed for training: {} seconds\\n\".format(timeSince(start)))\n",
    "start = time.time_ns()\n",
    "y_pred = pipe.predict(X_eval)\n",
    "print(\"Time elapsed for inference (eval set): {} seconds\\n\".format(timeSince(start)))\n",
    "print(classification_report(y_eval,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregando embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "MODEL = \"distilbert-videogame-descriptions-rating\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "def sentence_clf_output(text):\n",
    "    \"\"\"retorna el SequenceClassifierOutput\"\"\"\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input, return_dict=True, output_hidden_states=True)\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versión logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_embedding(clf_output):\n",
    "    # retorna el vector de scores de clasificacion (antes de la capa softmax)\n",
    "    return clf_output['logits'][0].detach().numpy().reshape(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LogitsEmbedding(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        embed = lambda row: logits_embedding(sentence_clf_output(row))\n",
    "        X_new = X.apply(embed)\n",
    "        X_new = np.concatenate(X_new.values)\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesisng_logits = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('BoC-plat',boc_some_values,'platforms'),\n",
    "        ('BoC-cat',boc_some_values,'categories'),\n",
    "        ('BoC-genres',boc_some_values,'genres'),\n",
    "        ('BoC-tags',boc_some_values,'tags'),\n",
    "\n",
    "        ('BoC-dev',boc_many_values,'developer'),\n",
    "        ('BoC-pub',boc_many_values,'publisher'),\n",
    "\n",
    "        ('OneHotEncoder',OneHotEncoder(handle_unknown='ignore'),['month']),\n",
    "        # ('StandardScaler',StandardScaler(), ['...']),\n",
    "        ('MinMaxScaler',MinMaxScaler(),['required_age','price']),\n",
    "        ('BoxCox',PowerTransformer(method='yeo-johnson'),['achievements','average_playtime','revenue']),\n",
    "        # ('unchanged',None,['english'])  # chequear como no hacer nada\n",
    "\n",
    "        ('LogitsText',LogitsEmbedding(),'short_description')\n",
    "])\n",
    "\n",
    "pipe_logits = Pipeline([\n",
    "    ('Pre-procesamiento',preprocesisng_logits),\n",
    "    (\"selector\", SelectPercentile(f_classif, percentile=95)),\n",
    "    ('Clasificador',MLPClassifier(early_stopping =True,max_iter = 100, random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados clasificación MLP con logit embeddings\n",
      "\n",
      "Time elapsed for training: 131.765849573 seconds\n",
      "\n",
      "Time elapsed for inference (eval set): 51.816425677000005 seconds\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.27      0.37      0.31       497\n",
      "Mostly Positive       0.26      0.21      0.23       512\n",
      "       Negative       0.45      0.35      0.39       387\n",
      "       Positive       0.35      0.41      0.38       610\n",
      "  Very Positive       0.42      0.32      0.36       359\n",
      "\n",
      "       accuracy                           0.33      2365\n",
      "      macro avg       0.35      0.33      0.33      2365\n",
      "   weighted avg       0.34      0.33      0.33      2365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados clasificación MLP con logit embeddings\\n\")\n",
    "\n",
    "start = time.time_ns()\n",
    "pipe_logits.fit(X_train, y_train)\n",
    "print(\"Time elapsed for training: {} seconds\\n\".format(timeSince(start)))\n",
    "start = time.time_ns()\n",
    "y_pred = pipe_logits.predict(X_eval)\n",
    "print(\"Time elapsed for inference (eval set): {} seconds\\n\".format(timeSince(start)))\n",
    "\n",
    "print(classification_report(y_eval,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versión token [CLF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_tok_embedding(cfl_output):\n",
    "    # retorna un numpy array correspondiente al token contextualizado\n",
    "    return cfl_output['hidden_states'][-1][0][0].detach().numpy().reshape(1,768)\n",
    "\n",
    "class CLFTokenEmbedding(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        embed = lambda row: first_tok_embedding(sentence_clf_output(row))\n",
    "        X_new = X.apply(embed)\n",
    "        X_new = np.concatenate(X_new.values)\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesisng_CLFToken = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('BoC-plat',boc_some_values,'platforms'),\n",
    "        ('BoC-cat',boc_some_values,'categories'),\n",
    "        ('BoC-genres',boc_some_values,'genres'),\n",
    "        ('BoC-tags',boc_some_values,'tags'),\n",
    "\n",
    "        ('BoC-dev',boc_many_values,'developer'),\n",
    "        ('BoC-pub',boc_many_values,'publisher'),\n",
    "\n",
    "        ('OneHotEncoder',OneHotEncoder(handle_unknown='ignore'),['month']),\n",
    "        # ('StandardScaler',StandardScaler(), ['...']),\n",
    "        ('MinMaxScaler',MinMaxScaler(),['required_age','price']),\n",
    "        ('BoxCox',PowerTransformer(method='yeo-johnson'),['achievements','average_playtime','revenue']),\n",
    "        # ('unchanged',None,['english'])  # chequear como no hacer nada\n",
    "\n",
    "        ('LogitsText',CLFTokenEmbedding(),'short_description')\n",
    "])\n",
    "\n",
    "pipe_CLFToken = Pipeline([\n",
    "    ('Pre-procesamiento',preprocesisng_CLFToken),\n",
    "    (\"selector\", SelectPercentile(f_classif, percentile=95)),\n",
    "    ('Clasificador',MLPClassifier(early_stopping =True,max_iter = 100, random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados clasificación MLP con CLFToken embeddings\n",
      "\n",
      "Time elapsed for training: 141.993939402 seconds\n",
      "\n",
      "Time elapsed for inference (eval set): 56.33335626 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados clasificación MLP con CLFToken embeddings\\n\")\n",
    "\n",
    "start = time.time_ns()\n",
    "pipe_CLFToken.fit(X_train, y_train)\n",
    "print(\"Time elapsed for training: {} seconds\\n\".format(timeSince(start)))\n",
    "start = time.time_ns()\n",
    "y_pred = pipe_CLFToken.predict(X_eval)\n",
    "print(\"Time elapsed for inference (eval set): {} seconds\\n\".format(timeSince(start)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 min approx entre fit y predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.29      0.31      0.30       497\n",
      "Mostly Positive       0.23      0.08      0.12       512\n",
      "       Negative       0.36      0.37      0.37       387\n",
      "       Positive       0.32      0.52      0.40       610\n",
      "  Very Positive       0.38      0.27      0.32       359\n",
      "\n",
      "       accuracy                           0.32      2365\n",
      "      macro avg       0.32      0.31      0.30      2365\n",
      "   weighted avg       0.31      0.32      0.30      2365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_eval,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versión promedio de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_embedding(cfl_output):\n",
    "    # retorna un numpy array correspondiente a la suma de los vectores contextualizados\n",
    "    return cfl_output['hidden_states'][-1][0].detach().numpy().mean(axis=0).reshape(1,768)\n",
    "\n",
    "class MeanEmbedding(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        embed = lambda row: mean_embedding(sentence_clf_output(row))\n",
    "        X_new = X.apply(embed)\n",
    "        X_new = np.concatenate(X_new.values)\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesisng_mean = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('BoC-plat',boc_some_values,'platforms'),\n",
    "        ('BoC-cat',boc_some_values,'categories'),\n",
    "        ('BoC-genres',boc_some_values,'genres'),\n",
    "        ('BoC-tags',boc_some_values,'tags'),\n",
    "\n",
    "        ('BoC-dev',boc_many_values,'developer'),\n",
    "        ('BoC-pub',boc_many_values,'publisher'),\n",
    "\n",
    "        ('OneHotEncoder',OneHotEncoder(handle_unknown='ignore'),['month']),\n",
    "        # ('StandardScaler',StandardScaler(), ['...']),\n",
    "        ('MinMaxScaler',MinMaxScaler(),['required_age','price']),\n",
    "        ('BoxCox',PowerTransformer(method='yeo-johnson'),['achievements','average_playtime','revenue']),\n",
    "        # ('unchanged',None,['english'])  # chequear como no hacer nada\n",
    "\n",
    "        ('LogitsText',MeanEmbedding(),'short_description')\n",
    "])\n",
    "\n",
    "pipe_mean = Pipeline([\n",
    "    ('Pre-procesamiento',preprocesisng_mean),\n",
    "    (\"selector\", SelectPercentile(f_classif, percentile=95)),\n",
    "    ('Clasificador',MLPClassifier(early_stopping =True,max_iter = 100, random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados clasificación MLP con mean embeddings\n",
      "\n",
      "Time elapsed for training: 139.731334318 seconds\n",
      "\n",
      "Time elapsed for inference (eval set): 59.623988174000004 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados clasificación MLP con mean embeddings\\n\")\n",
    "\n",
    "start = time.time_ns()\n",
    "pipe_mean.fit(X_train, y_train)\n",
    "print(\"Time elapsed for training: {} seconds\\n\".format(timeSince(start)))\n",
    "start = time.time_ns()\n",
    "y_pred = pipe_mean.predict(X_eval)\n",
    "print(\"Time elapsed for inference (eval set): {} seconds\\n\".format(timeSince(start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.27      0.37      0.31       497\n",
      "Mostly Positive       0.20      0.08      0.11       512\n",
      "       Negative       0.34      0.39      0.36       387\n",
      "       Positive       0.34      0.47      0.39       610\n",
      "  Very Positive       0.39      0.21      0.27       359\n",
      "\n",
      "       accuracy                           0.31      2365\n",
      "      macro avg       0.31      0.30      0.29      2365\n",
      "   weighted avg       0.30      0.31      0.29      2365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_eval,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words clásicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Definimos un tokenizador con Stemming\n",
    "class StemmerTokenizer:\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        doc_tok = word_tokenize(doc)\n",
    "        doc_tok = [t for t in doc_tok if t not in stop_words]\n",
    "        return [self.ps.stem(t) for t in doc_tok]\n",
    "\n",
    "bow = CountVectorizer(\n",
    "    tokenizer= StemmerTokenizer(),\n",
    "    ngram_range=(1,2),\n",
    "    min_df = 0.05, max_df = 0.85\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesisng_bow = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('BoC-plat',boc_some_values,'platforms'),\n",
    "        ('BoC-cat',boc_some_values,'categories'),\n",
    "        ('BoC-genres',boc_some_values,'genres'),\n",
    "        ('BoC-tags',boc_some_values,'tags'),\n",
    "\n",
    "        ('BoC-dev',boc_many_values,'developer'),\n",
    "        ('BoC-pub',boc_many_values,'publisher'),\n",
    "\n",
    "        ('OneHotEncoder',OneHotEncoder(handle_unknown='ignore'),['month']),\n",
    "        # ('StandardScaler',StandardScaler(), ['...']),\n",
    "        ('MinMaxScaler',MinMaxScaler(),['required_age','price','release_date']),\n",
    "        ('BoxCox',PowerTransformer(method='yeo-johnson'),['achievements','average_playtime','revenue']),\n",
    "        # ('unchanged',None,['english'])  # chequear como no hacer nada\n",
    "\n",
    "        ('BoWText',bow,'short_description')\n",
    "])\n",
    "\n",
    "pipe_bow = Pipeline([\n",
    "    ('Pre-procesamiento',preprocesisng_bow),\n",
    "    (\"selector\", SelectPercentile(f_classif, percentile=95)),\n",
    "    ('Clasificador',MLPClassifier(early_stopping =True,max_iter = 100, random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados clasificación MLP con bag-of-words\n",
      "\n",
      "Time elapsed for training: 8.205354385 seconds\n",
      "\n",
      "Time elapsed for inference (eval set): 1.5025425110000001 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados clasificación MLP con bag-of-words\\n\")\n",
    "\n",
    "start = time.time_ns()\n",
    "pipe_bow.fit(X_train, y_train)\n",
    "print(\"Time elapsed for training: {} seconds\\n\".format(timeSince(start)))\n",
    "start = time.time_ns()\n",
    "y_pred = pipe_bow.predict(X_eval)\n",
    "print(\"Time elapsed for inference (eval set): {} seconds\\n\".format(timeSince(start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.31      0.24      0.27       497\n",
      "Mostly Positive       0.29      0.17      0.21       512\n",
      "       Negative       0.43      0.41      0.42       387\n",
      "       Positive       0.32      0.52      0.39       610\n",
      "  Very Positive       0.43      0.35      0.38       359\n",
      "\n",
      "       accuracy                           0.34      2365\n",
      "      macro avg       0.35      0.34      0.34      2365\n",
      "   weighted avg       0.34      0.34      0.33      2365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_eval,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW + logits..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados clasificación MLP con logit embeddings + BoW\n",
      "\n",
      "Time elapsed for training: 136.28623183300002 seconds\n",
      "\n",
      "Time elapsed for inference (eval set): 53.621551404 seconds\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Mixed       0.28      0.30      0.29       497\n",
      "Mostly Positive       0.26      0.17      0.21       512\n",
      "       Negative       0.38      0.48      0.42       387\n",
      "       Positive       0.35      0.41      0.38       610\n",
      "  Very Positive       0.42      0.36      0.38       359\n",
      "\n",
      "       accuracy                           0.34      2365\n",
      "      macro avg       0.34      0.34      0.34      2365\n",
      "   weighted avg       0.33      0.34      0.33      2365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocesisng_logits_bow = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('BoC-plat',boc_some_values,'platforms'),\n",
    "        ('BoC-cat',boc_some_values,'categories'),\n",
    "        ('BoC-genres',boc_some_values,'genres'),\n",
    "        ('BoC-tags',boc_some_values,'tags'),\n",
    "\n",
    "        ('BoC-dev',boc_many_values,'developer'),\n",
    "        ('BoC-pub',boc_many_values,'publisher'),\n",
    "\n",
    "        ('OneHotEncoder',OneHotEncoder(handle_unknown='ignore'),['month']),\n",
    "        # ('StandardScaler',StandardScaler(), ['...']),\n",
    "        ('MinMaxScaler',MinMaxScaler(),['required_age','price','release_date']),\n",
    "        ('BoxCox',PowerTransformer(method='yeo-johnson'),['achievements','average_playtime','revenue']),\n",
    "        # ('unchanged',None,['english'])  # chequear como no hacer nada\n",
    "\n",
    "        ('LogitsText',LogitsEmbedding(),'short_description')\n",
    "])\n",
    "\n",
    "pipe_logits_bow = Pipeline([\n",
    "    ('Pre-procesamiento',preprocesisng_logits_bow),\n",
    "    (\"selector\", SelectPercentile(f_classif, percentile=95)),\n",
    "    ('Clasificador',MLPClassifier(early_stopping =True,max_iter = 100, random_state=0))\n",
    "])\n",
    "\n",
    "\n",
    "print(\"Resultados clasificación MLP con logit embeddings + BoW\\n\")\n",
    "\n",
    "start = time.time_ns()\n",
    "pipe_logits_bow.fit(X_train, y_train)\n",
    "print(\"Time elapsed for training: {} seconds\\n\".format(timeSince(start)))\n",
    "start = time.time_ns()\n",
    "y_pred = pipe_logits_bow.predict(X_eval)\n",
    "print(\"Time elapsed for inference (eval set): {} seconds\\n\".format(timeSince(start)))\n",
    "\n",
    "print(classification_report(y_eval,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('train.pickle')\n",
    "df_train = custom_features(df_train)\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(df_train, df_train['estimated_sells'], test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados regresión sin texto\n",
      "\n",
      "Time elapsed for training: 9.014024873 seconds\n",
      "\n",
      "Time elapsed for inference (eval set): 0.057883677 seconds\n",
      "\n",
      "Error cuadrático medio = 730686091203.4044\n",
      "Score R2 = 0.5924228283913218\n"
     ]
    }
   ],
   "source": [
    "pipe_reg = Pipeline([\n",
    "    ('Pre-procesamiento',preprocesisng),\n",
    "    (\"selector\", SelectPercentile(f_classif, percentile=95)),\n",
    "    ('Regresor',BaggingRegressor(random_state=0))\n",
    "])\n",
    "\n",
    "print(\"Resultados regresión sin texto\\n\")\n",
    "\n",
    "start = time.time_ns()\n",
    "pipe_reg.fit(X_train, y_train)\n",
    "print(\"Time elapsed for training: {} seconds\\n\".format(timeSince(start)))\n",
    "start = time.time_ns()\n",
    "y_pred = pipe_reg.predict(X_eval)\n",
    "print(\"Time elapsed for inference (eval set): {} seconds\\n\".format(timeSince(start)))\n",
    "\n",
    "print(\"Error cuadrático medio = {}\".format(mean_squared_error(y_eval,y_pred)))\n",
    "print(\"Score R2 = {}\".format(r2_score(y_eval,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "MODEL = \"distilbert-videogames-descriptions-sells\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "def sentence_clf_output(text):\n",
    "    \"\"\"retorna el SequenceClassifierOutput\"\"\"\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input, return_dict=True, output_hidden_states=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_reg_bow = Pipeline([\n",
    "    ('Pre-procesamiento',preprocesisng_bow),\n",
    "    (\"selector\", SelectPercentile(f_classif, percentile=95)),\n",
    "    ('Regresor',BaggingRegressor(random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados regresión con bag-of-words\n",
      "\n",
      "Time elapsed for training: 11.977677203 seconds\n",
      "\n",
      "Time elapsed for inference (eval set): 1.4346816690000002 seconds\n",
      "\n",
      "Error cuadrático medio = 824150143306.8201\n",
      "Score R2 = 0.5402885200173145\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados regresión con bag-of-words\\n\")\n",
    "\n",
    "start = time.time_ns()\n",
    "pipe_reg_bow.fit(X_train, y_train)\n",
    "print(\"Time elapsed for training: {} seconds\\n\".format(timeSince(start)))\n",
    "start = time.time_ns()\n",
    "y_pred = pipe_reg_bow.predict(X_eval)\n",
    "print(\"Time elapsed for inference (eval set): {} seconds\\n\".format(timeSince(start)))\n",
    "\n",
    "print(\"Error cuadrático medio = {}\".format(mean_squared_error(y_eval,y_pred)))\n",
    "print(\"Score R2 = {}\".format(r2_score(y_eval,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_embedding(clf_output):\n",
    "    # retorna el vector de scores de clasificacion (antes de la capa softmax)\n",
    "    return clf_output['logits'][0].detach().numpy().reshape(1,1)\n",
    "\n",
    "class LogitsEmbedding(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        embed = lambda row: logits_embedding(sentence_clf_output(row))\n",
    "        X_new = X.apply(embed)\n",
    "        X_new = np.concatenate(X_new.values)\n",
    "        return X_new\n",
    "\n",
    "preprocesisng_logits_reg = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('BoC-plat',boc_some_values,'platforms'),\n",
    "        ('BoC-cat',boc_some_values,'categories'),\n",
    "        ('BoC-genres',boc_some_values,'genres'),\n",
    "        ('BoC-tags',boc_some_values,'tags'),\n",
    "\n",
    "        ('BoC-dev',boc_many_values,'developer'),\n",
    "        ('BoC-pub',boc_many_values,'publisher'),\n",
    "\n",
    "        ('OneHotEncoder',OneHotEncoder(handle_unknown='ignore'),['month']),\n",
    "        # ('StandardScaler',StandardScaler(), ['...']),\n",
    "        ('MinMaxScaler',MinMaxScaler(),['required_age','price','release_date']),\n",
    "        ('BoxCox',PowerTransformer(method='yeo-johnson'),['achievements','average_playtime','revenue']),\n",
    "        # ('unchanged',None,['english'])  # chequear como no hacer nada\n",
    "\n",
    "        ('LogitsText',LogitsEmbedding(),'short_description')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_reg_logits = Pipeline([\n",
    "    ('Pre-procesamiento',preprocesisng_logits_reg),\n",
    "    (\"selector\", SelectPercentile(f_classif, percentile=95)),\n",
    "    ('Regresor',BaggingRegressor(random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados regresión con logit embeddings\n",
      "\n",
      "Time elapsed for training: 129.489771709 seconds\n",
      "\n",
      "Time elapsed for inference (eval set): 52.863194716 seconds\n",
      "\n",
      "Error cuadrático medio = 784042788295.7003\n",
      "Score R2 = 0.5626604284372694\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados regresión con logit embeddings\\n\")\n",
    "\n",
    "start = time.time_ns()\n",
    "pipe_reg_logits.fit(X_train, y_train)\n",
    "print(\"Time elapsed for training: {} seconds\\n\".format(timeSince(start)))\n",
    "start = time.time_ns()\n",
    "y_pred = pipe_reg_logits.predict(X_eval)\n",
    "print(\"Time elapsed for inference (eval set): {} seconds\\n\".format(timeSince(start)))\n",
    "\n",
    "print(\"Error cuadrático medio = {}\".format(mean_squared_error(y_eval,y_pred)))\n",
    "print(\"Score R2 = {}\".format(r2_score(y_eval,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error cuadrático medio = 908859779499.0122\n",
      "Score R2 = 0.49303743046892634\n"
     ]
    }
   ],
   "source": [
    "print(\"Error cuadrático medio = {}\".format(mean_squared_error(y_eval,y_pred)))\n",
    "print(\"Score R2 = {}\".format(r2_score(y_eval,y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prog_cientifica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f12cc99d524414e3daaca7ff7e8e0997cdecd19277513297c62237218c0d43b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
